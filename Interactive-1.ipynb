{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3 (ipykernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36255e-9b18-4d57-9bac-eb7ba691c18b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c814990-11fb-40c8-bcaf-3ceaa313e3bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce99c85-37a8-4d76-9777-35db54069395",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Loading Data\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# load the data, column titles in salary in first row\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m salary_data = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33m2025_salaries.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m stats = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mnba_2025.txt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "# Merging Data\n",
    "# sklearn\n",
    "\n",
    "# load the data, column titles in salary in first row\n",
    "salary_data = pd.read_csv(\"2025_salaries.csv\")\n",
    "stats = pd.read_csv(\"nba_2025.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17d15b-054c-4dfd-a007-09ce0d588b21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859173a-a02f-4387-b643-b1058983c374",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1eebdb-043c-4b48-ab01-effc9581748b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76581d8f-3c54-41f1-adcf-9f28dd84ff64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc8532-474d-4a2f-b308-d6d99938f6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebac161-28bc-4684-9bcd-2cd105a1d44d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'executable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRUNNING:\u001b[39m\u001b[33m\"\u001b[39m, sys,\u001b[43mexecutable\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'executable' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"RUNNING:\", sys,executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING: /usr/local/python/3.12.1/bin/python\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"RUNNING:\", sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to venv (3.12.1) (Python 3.12.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee07f3-2a74-4830-9e08-0241ed15e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b0d51-9762-443a-a11c-b6b75d7846f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "# Merging Data\n",
    "# sklearn\n",
    "\n",
    "# load the data, column titles in salary in first row\n",
    "salary_data = pd.read_csv(\"2025_salaries.csv\")\n",
    "stats = pd.read_csv(\"nba_2025.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d5099-8196-4666-a630-e8d80efc3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.head()\n",
    "\n",
    "stats = pd.read_csv(\"nba_2025.txt\", sep=\",\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82d7d1-b82d-43bf-9af6-8d41f73f7ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function merge in module pandas:\n",
      "\n",
      "merge(left: 'DataFrame | Series', right: 'DataFrame | Series', how: 'MergeHow' = 'inner', on: 'IndexLabel | AnyArrayLike | None' = None, left_on: 'IndexLabel | AnyArrayLike | None' = None, right_on: 'IndexLabel | AnyArrayLike | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool | lib.NoDefault' = <no_default>, indicator: 'str | bool' = False, validate: 'str | None' = None) -> 'DataFrame'\n",
      "    Merge DataFrame or named Series objects with a database-style join.\n",
      "\n",
      "    A named Series object is treated as a DataFrame with a single named column.\n",
      "\n",
      "    The join is done on columns or indexes. If joining columns on\n",
      "    columns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\n",
      "    on indexes or indexes on a column or columns, the index will be passed on.\n",
      "    When performing a cross merge, no column specifications to merge on are\n",
      "    allowed.\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        If both key columns contain rows where the key is a null value, those\n",
      "        rows will be matched against each other. This is different from usual SQL\n",
      "        join behaviour and can lead to unexpected results.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    left : DataFrame or named Series\n",
      "        First pandas object to merge.\n",
      "    right : DataFrame or named Series\n",
      "        Second pandas object to merge.\n",
      "    how : {'left', 'right', 'outer', 'inner', 'cross', 'left_anti', 'right_anti},\n",
      "        default 'inner'\n",
      "        Type of merge to be performed.\n",
      "\n",
      "        * left: use only keys from left frame, similar to a SQL left outer join;\n",
      "          preserve key order.\n",
      "        * right: use only keys from right frame, similar to a SQL right outer join;\n",
      "          preserve key order.\n",
      "        * outer: use union of keys from both frames, similar to a SQL full outer\n",
      "          join; sort keys lexicographically.\n",
      "        * inner: use intersection of keys from both frames, similar to a SQL inner\n",
      "          join; preserve the order of the left keys.\n",
      "        * cross: creates the cartesian product from both frames, preserves the order\n",
      "          of the left keys.\n",
      "        * left_anti: use only keys from left frame that are not in right frame, similar\n",
      "          to SQL left anti join; preserve key order.\n",
      "        * right_anti: use only keys from right frame that are not in left frame, similar\n",
      "          to SQL right anti join; preserve key order.\n",
      "    on : Hashable or a sequence of the previous\n",
      "        Column or index level names to join on. These must be found in both\n",
      "        DataFrames. If `on` is None and not merging on indexes then this defaults\n",
      "        to the intersection of the columns in both DataFrames.\n",
      "    left_on : Hashable or a sequence of the previous, or array-like\n",
      "        Column or index level names to join on in the left DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the left DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    right_on : Hashable or a sequence of the previous, or array-like\n",
      "        Column or index level names to join on in the right DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the right DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    left_index : bool, default False\n",
      "        Use the index from the left DataFrame as the join key(s). If it is a\n",
      "        MultiIndex, the number of keys in the other DataFrame (either the index\n",
      "        or a number of columns) must match the number of levels.\n",
      "    right_index : bool, default False\n",
      "        Use the index from the right DataFrame as the join key. Same caveats as\n",
      "        left_index.\n",
      "    sort : bool, default False\n",
      "        Sort the join keys lexicographically in the result DataFrame. If False,\n",
      "        the order of the join keys depends on the join type (how keyword).\n",
      "    suffixes : list-like, default is (\"_x\", \"_y\")\n",
      "        A length-2 sequence where each element is optionally a string\n",
      "        indicating the suffix to add to overlapping column names in\n",
      "        `left` and `right` respectively. Pass a value of `None` instead\n",
      "        of a string to indicate that the column name from `left` or\n",
      "        `right` should be left as-is, with no suffix. At least one of the\n",
      "        values must not be None.\n",
      "    copy : bool, default False\n",
      "        This keyword is now ignored; changing its value will have no\n",
      "        impact on the method.\n",
      "\n",
      "        .. deprecated:: 3.0.0\n",
      "\n",
      "            This keyword is ignored and will be removed in pandas 4.0. Since\n",
      "            pandas 3.0, this method always returns a new object using a lazy\n",
      "            copy mechanism that defers copies until necessary\n",
      "            (Copy-on-Write). See the `user guide on Copy-on-Write\n",
      "            <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      "            for more details.\n",
      "\n",
      "    indicator : bool or str, default False\n",
      "        If True, adds a column to the output DataFrame called \"_merge\" with\n",
      "        information on the source of each row. The column can be given a different\n",
      "        name by providing a string argument. The column will have a Categorical\n",
      "        type with the value of \"left_only\" for observations whose merge key only\n",
      "        appears in the left DataFrame, \"right_only\" for observations\n",
      "        whose merge key only appears in the right DataFrame, and \"both\"\n",
      "        if the observation's merge key is found in both DataFrames.\n",
      "\n",
      "    validate : str, optional\n",
      "        If specified, checks if merge is of specified type.\n",
      "\n",
      "        * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n",
      "          left and right datasets.\n",
      "        * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n",
      "          dataset.\n",
      "        * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n",
      "          dataset.\n",
      "        * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        A DataFrame of the two merged objects.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    merge_ordered : Merge with optional filling/interpolation.\n",
      "    merge_asof : Merge on nearest keys.\n",
      "    DataFrame.join : Similar method using indices.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> df1 = pd.DataFrame(\n",
      "    ...     {\"lkey\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [1, 2, 3, 5]}\n",
      "    ... )\n",
      "    >>> df2 = pd.DataFrame(\n",
      "    ...     {\"rkey\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [5, 6, 7, 8]}\n",
      "    ... )\n",
      "    >>> df1\n",
      "        lkey value\n",
      "    0   foo      1\n",
      "    1   bar      2\n",
      "    2   baz      3\n",
      "    3   foo      5\n",
      "    >>> df2\n",
      "        rkey value\n",
      "    0   foo      5\n",
      "    1   bar      6\n",
      "    2   baz      7\n",
      "    3   foo      8\n",
      "\n",
      "    Merge df1 and df2 on the lkey and rkey columns. The value columns have\n",
      "    the default suffixes, _x and _y, appended.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\")\n",
      "      lkey  value_x rkey  value_y\n",
      "    0  foo        1  foo        5\n",
      "    1  foo        1  foo        8\n",
      "    2  bar        2  bar        6\n",
      "    3  baz        3  baz        7\n",
      "    4  foo        5  foo        5\n",
      "    5  foo        5  foo        8\n",
      "\n",
      "    Merge DataFrames df1 and df2 with specified left and right suffixes\n",
      "    appended to any overlapping columns.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\", suffixes=(\"_left\", \"_right\"))\n",
      "      lkey  value_left rkey  value_right\n",
      "    0  foo           1  foo            5\n",
      "    1  foo           1  foo            8\n",
      "    2  bar           2  bar            6\n",
      "    3  baz           3  baz            7\n",
      "    4  foo           5  foo            5\n",
      "    5  foo           5  foo            8\n",
      "\n",
      "    Merge DataFrames df1 and df2, but raise an exception if the DataFrames have\n",
      "    any overlapping columns.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\", suffixes=(False, False))\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: columns overlap but no suffix specified:\n",
      "        Index(['value'], dtype='str')\n",
      "\n",
      "    >>> df1 = pd.DataFrame({\"a\": [\"foo\", \"bar\"], \"b\": [1, 2]})\n",
      "    >>> df2 = pd.DataFrame({\"a\": [\"foo\", \"baz\"], \"c\": [3, 4]})\n",
      "    >>> df1\n",
      "          a  b\n",
      "    0   foo  1\n",
      "    1   bar  2\n",
      "    >>> df2\n",
      "          a  c\n",
      "    0   foo  3\n",
      "    1   baz  4\n",
      "\n",
      "    >>> df1.merge(df2, how=\"inner\", on=\"a\")\n",
      "          a  b  c\n",
      "    0   foo  1  3\n",
      "\n",
      "    >>> df1.merge(df2, how=\"left\", on=\"a\")\n",
      "          a  b  c\n",
      "    0   foo  1  3.0\n",
      "    1   bar  2  NaN\n",
      "\n",
      "    >>> df1 = pd.DataFrame({\"left\": [\"foo\", \"bar\"]})\n",
      "    >>> df2 = pd.DataFrame({\"right\": [7, 8]})\n",
      "    >>> df1\n",
      "        left\n",
      "    0   foo\n",
      "    1   bar\n",
      "    >>> df2\n",
      "        right\n",
      "    0   7\n",
      "    1   8\n",
      "\n",
      "    >>> df1.merge(df2, how=\"cross\")\n",
      "       left  right\n",
      "    0   foo      7\n",
      "    1   foo      8\n",
      "    2   bar      7\n",
      "    3   bar      8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1292ff2-6dce-4e59-947f-fa6474ebe4b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Player'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m<ipython-input-5-569420f5d4d7>\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m merged_data = pd.merge(salary_data, stats, on=\u001b[33m'Player'\u001b[39m)\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    381\u001b[39m             indicator=indicator,\n\u001b[32m    382\u001b[39m             validate=validate,\n\u001b[32m    383\u001b[39m         )\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         op = _MergeOperation(\n\u001b[32m    386\u001b[39m             left_df,\n\u001b[32m    387\u001b[39m             right_df,\n\u001b[32m    388\u001b[39m             how=how,\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m   1014\u001b[39m             self.right_join_keys,\n\u001b[32m   1015\u001b[39m             self.join_names,\n\u001b[32m   1016\u001b[39m             left_drop,\n\u001b[32m   1017\u001b[39m             right_drop,\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m   1019\u001b[39m \n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m   1021\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1629\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1630\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1631\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1632\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1634\u001b[39m                         join_names.append(lk)\n\u001b[32m   1635\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1636\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1772\u001b[39m             values = self.xs(key, axis=first_other_axes)._values\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1774\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1775\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1777\u001b[39m \n\u001b[32m   1778\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1779\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Player'"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(salary_data, stats, on='Player')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6295972-f0bb-43b0-9001-7535ce21b841",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Player'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m<ipython-input-6-669357ef2043>\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m merged_data = pd.merge(salary_data, stats, on=\u001b[33m'Player'\u001b[39m)\n\u001b[32m      3\u001b[39m merged_data.head()\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    381\u001b[39m             indicator=indicator,\n\u001b[32m    382\u001b[39m             validate=validate,\n\u001b[32m    383\u001b[39m         )\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         op = _MergeOperation(\n\u001b[32m    386\u001b[39m             left_df,\n\u001b[32m    387\u001b[39m             right_df,\n\u001b[32m    388\u001b[39m             how=how,\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m   1014\u001b[39m             self.right_join_keys,\n\u001b[32m   1015\u001b[39m             self.join_names,\n\u001b[32m   1016\u001b[39m             left_drop,\n\u001b[32m   1017\u001b[39m             right_drop,\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m   1019\u001b[39m \n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m   1021\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1629\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1630\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1631\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1632\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1634\u001b[39m                         join_names.append(lk)\n\u001b[32m   1635\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1636\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1772\u001b[39m             values = self.xs(key, axis=first_other_axes)._values\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1774\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1775\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1777\u001b[39m \n\u001b[32m   1778\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1779\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Player'"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(salary_data, stats, on='Player')\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cee34-c9f4-494e-aa96-e68bfa12e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b47db5-3300-4e33-b345-4051b6ac2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "# Merging Data\n",
    "# sklearn\n",
    "\n",
    "# load the data, column titles in salary in first row\n",
    "salary_data = pd.read_csv(\"2025_salaries.csv\")\n",
    "stats = pd.read_csv(\"nba_2025.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08237ad1-a13f-4c44-b75a-28791d676e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.head()\n",
    "\n",
    "stats = pd.read_csv(\"nba_2025.txt\", sep=\",\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33573f75-3143-467a-84fe-3f2563b22915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function merge in module pandas:\n",
      "\n",
      "merge(left: 'DataFrame | Series', right: 'DataFrame | Series', how: 'MergeHow' = 'inner', on: 'IndexLabel | AnyArrayLike | None' = None, left_on: 'IndexLabel | AnyArrayLike | None' = None, right_on: 'IndexLabel | AnyArrayLike | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool | lib.NoDefault' = <no_default>, indicator: 'str | bool' = False, validate: 'str | None' = None) -> 'DataFrame'\n",
      "    Merge DataFrame or named Series objects with a database-style join.\n",
      "\n",
      "    A named Series object is treated as a DataFrame with a single named column.\n",
      "\n",
      "    The join is done on columns or indexes. If joining columns on\n",
      "    columns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\n",
      "    on indexes or indexes on a column or columns, the index will be passed on.\n",
      "    When performing a cross merge, no column specifications to merge on are\n",
      "    allowed.\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        If both key columns contain rows where the key is a null value, those\n",
      "        rows will be matched against each other. This is different from usual SQL\n",
      "        join behaviour and can lead to unexpected results.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    left : DataFrame or named Series\n",
      "        First pandas object to merge.\n",
      "    right : DataFrame or named Series\n",
      "        Second pandas object to merge.\n",
      "    how : {'left', 'right', 'outer', 'inner', 'cross', 'left_anti', 'right_anti},\n",
      "        default 'inner'\n",
      "        Type of merge to be performed.\n",
      "\n",
      "        * left: use only keys from left frame, similar to a SQL left outer join;\n",
      "          preserve key order.\n",
      "        * right: use only keys from right frame, similar to a SQL right outer join;\n",
      "          preserve key order.\n",
      "        * outer: use union of keys from both frames, similar to a SQL full outer\n",
      "          join; sort keys lexicographically.\n",
      "        * inner: use intersection of keys from both frames, similar to a SQL inner\n",
      "          join; preserve the order of the left keys.\n",
      "        * cross: creates the cartesian product from both frames, preserves the order\n",
      "          of the left keys.\n",
      "        * left_anti: use only keys from left frame that are not in right frame, similar\n",
      "          to SQL left anti join; preserve key order.\n",
      "        * right_anti: use only keys from right frame that are not in left frame, similar\n",
      "          to SQL right anti join; preserve key order.\n",
      "    on : Hashable or a sequence of the previous\n",
      "        Column or index level names to join on. These must be found in both\n",
      "        DataFrames. If `on` is None and not merging on indexes then this defaults\n",
      "        to the intersection of the columns in both DataFrames.\n",
      "    left_on : Hashable or a sequence of the previous, or array-like\n",
      "        Column or index level names to join on in the left DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the left DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    right_on : Hashable or a sequence of the previous, or array-like\n",
      "        Column or index level names to join on in the right DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the right DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    left_index : bool, default False\n",
      "        Use the index from the left DataFrame as the join key(s). If it is a\n",
      "        MultiIndex, the number of keys in the other DataFrame (either the index\n",
      "        or a number of columns) must match the number of levels.\n",
      "    right_index : bool, default False\n",
      "        Use the index from the right DataFrame as the join key. Same caveats as\n",
      "        left_index.\n",
      "    sort : bool, default False\n",
      "        Sort the join keys lexicographically in the result DataFrame. If False,\n",
      "        the order of the join keys depends on the join type (how keyword).\n",
      "    suffixes : list-like, default is (\"_x\", \"_y\")\n",
      "        A length-2 sequence where each element is optionally a string\n",
      "        indicating the suffix to add to overlapping column names in\n",
      "        `left` and `right` respectively. Pass a value of `None` instead\n",
      "        of a string to indicate that the column name from `left` or\n",
      "        `right` should be left as-is, with no suffix. At least one of the\n",
      "        values must not be None.\n",
      "    copy : bool, default False\n",
      "        This keyword is now ignored; changing its value will have no\n",
      "        impact on the method.\n",
      "\n",
      "        .. deprecated:: 3.0.0\n",
      "\n",
      "            This keyword is ignored and will be removed in pandas 4.0. Since\n",
      "            pandas 3.0, this method always returns a new object using a lazy\n",
      "            copy mechanism that defers copies until necessary\n",
      "            (Copy-on-Write). See the `user guide on Copy-on-Write\n",
      "            <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      "            for more details.\n",
      "\n",
      "    indicator : bool or str, default False\n",
      "        If True, adds a column to the output DataFrame called \"_merge\" with\n",
      "        information on the source of each row. The column can be given a different\n",
      "        name by providing a string argument. The column will have a Categorical\n",
      "        type with the value of \"left_only\" for observations whose merge key only\n",
      "        appears in the left DataFrame, \"right_only\" for observations\n",
      "        whose merge key only appears in the right DataFrame, and \"both\"\n",
      "        if the observation's merge key is found in both DataFrames.\n",
      "\n",
      "    validate : str, optional\n",
      "        If specified, checks if merge is of specified type.\n",
      "\n",
      "        * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n",
      "          left and right datasets.\n",
      "        * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n",
      "          dataset.\n",
      "        * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n",
      "          dataset.\n",
      "        * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        A DataFrame of the two merged objects.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    merge_ordered : Merge with optional filling/interpolation.\n",
      "    merge_asof : Merge on nearest keys.\n",
      "    DataFrame.join : Similar method using indices.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> df1 = pd.DataFrame(\n",
      "    ...     {\"lkey\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [1, 2, 3, 5]}\n",
      "    ... )\n",
      "    >>> df2 = pd.DataFrame(\n",
      "    ...     {\"rkey\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [5, 6, 7, 8]}\n",
      "    ... )\n",
      "    >>> df1\n",
      "        lkey value\n",
      "    0   foo      1\n",
      "    1   bar      2\n",
      "    2   baz      3\n",
      "    3   foo      5\n",
      "    >>> df2\n",
      "        rkey value\n",
      "    0   foo      5\n",
      "    1   bar      6\n",
      "    2   baz      7\n",
      "    3   foo      8\n",
      "\n",
      "    Merge df1 and df2 on the lkey and rkey columns. The value columns have\n",
      "    the default suffixes, _x and _y, appended.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\")\n",
      "      lkey  value_x rkey  value_y\n",
      "    0  foo        1  foo        5\n",
      "    1  foo        1  foo        8\n",
      "    2  bar        2  bar        6\n",
      "    3  baz        3  baz        7\n",
      "    4  foo        5  foo        5\n",
      "    5  foo        5  foo        8\n",
      "\n",
      "    Merge DataFrames df1 and df2 with specified left and right suffixes\n",
      "    appended to any overlapping columns.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\", suffixes=(\"_left\", \"_right\"))\n",
      "      lkey  value_left rkey  value_right\n",
      "    0  foo           1  foo            5\n",
      "    1  foo           1  foo            8\n",
      "    2  bar           2  bar            6\n",
      "    3  baz           3  baz            7\n",
      "    4  foo           5  foo            5\n",
      "    5  foo           5  foo            8\n",
      "\n",
      "    Merge DataFrames df1 and df2, but raise an exception if the DataFrames have\n",
      "    any overlapping columns.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\", suffixes=(False, False))\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: columns overlap but no suffix specified:\n",
      "        Index(['value'], dtype='str')\n",
      "\n",
      "    >>> df1 = pd.DataFrame({\"a\": [\"foo\", \"bar\"], \"b\": [1, 2]})\n",
      "    >>> df2 = pd.DataFrame({\"a\": [\"foo\", \"baz\"], \"c\": [3, 4]})\n",
      "    >>> df1\n",
      "          a  b\n",
      "    0   foo  1\n",
      "    1   bar  2\n",
      "    >>> df2\n",
      "          a  c\n",
      "    0   foo  3\n",
      "    1   baz  4\n",
      "\n",
      "    >>> df1.merge(df2, how=\"inner\", on=\"a\")\n",
      "          a  b  c\n",
      "    0   foo  1  3\n",
      "\n",
      "    >>> df1.merge(df2, how=\"left\", on=\"a\")\n",
      "          a  b  c\n",
      "    0   foo  1  3.0\n",
      "    1   bar  2  NaN\n",
      "\n",
      "    >>> df1 = pd.DataFrame({\"left\": [\"foo\", \"bar\"]})\n",
      "    >>> df2 = pd.DataFrame({\"right\": [7, 8]})\n",
      "    >>> df1\n",
      "        left\n",
      "    0   foo\n",
      "    1   bar\n",
      "    >>> df2\n",
      "        right\n",
      "    0   7\n",
      "    1   8\n",
      "\n",
      "    >>> df1.merge(df2, how=\"cross\")\n",
      "       left  right\n",
      "    0   foo      7\n",
      "    1   foo      8\n",
      "    2   bar      7\n",
      "    3   bar      8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3b704-8d00-4490-94d0-62626e86f58e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Player'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m<ipython-input-11-669357ef2043>\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m merged_data = pd.merge(salary_data, stats, on=\u001b[33m'Player'\u001b[39m)\n\u001b[32m      3\u001b[39m merged_data.head()\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    381\u001b[39m             indicator=indicator,\n\u001b[32m    382\u001b[39m             validate=validate,\n\u001b[32m    383\u001b[39m         )\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         op = _MergeOperation(\n\u001b[32m    386\u001b[39m             left_df,\n\u001b[32m    387\u001b[39m             right_df,\n\u001b[32m    388\u001b[39m             how=how,\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m   1014\u001b[39m             self.right_join_keys,\n\u001b[32m   1015\u001b[39m             self.join_names,\n\u001b[32m   1016\u001b[39m             left_drop,\n\u001b[32m   1017\u001b[39m             right_drop,\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m   1019\u001b[39m \n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m   1021\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1629\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1630\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1631\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1632\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1634\u001b[39m                         join_names.append(lk)\n\u001b[32m   1635\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1636\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32m/workspaces/week_6/venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1772\u001b[39m             values = self.xs(key, axis=first_other_axes)._values\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1774\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1775\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1777\u001b[39m \n\u001b[32m   1778\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1779\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Player'"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(salary_data, stats, on='Player')\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e2afb-4ac3-493b-bee6-f00975fbc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "# Merging Data\n",
    "# sklearn\n",
    "\n",
    "# load the data, column titles in salary in first row\n",
    "salary_data = pd.read_csv(\"2025_salaries.csv\", header=1, encoding=\"latin-1\")\n",
    "stats = pd.read_csv(\"nba_2025.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e5d19-edd0-40b2-8654-c8a8b6cfa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.head()\n",
    "\n",
    "stats = pd.read_csv(\"nba_2025.txt\", sep=\",\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfcd77-b94d-4029-96da-a1d9fe762954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function merge in module pandas:\n",
      "\n",
      "merge(left: 'DataFrame | Series', right: 'DataFrame | Series', how: 'MergeHow' = 'inner', on: 'IndexLabel | AnyArrayLike | None' = None, left_on: 'IndexLabel | AnyArrayLike | None' = None, right_on: 'IndexLabel | AnyArrayLike | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool | lib.NoDefault' = <no_default>, indicator: 'str | bool' = False, validate: 'str | None' = None) -> 'DataFrame'\n",
      "    Merge DataFrame or named Series objects with a database-style join.\n",
      "\n",
      "    A named Series object is treated as a DataFrame with a single named column.\n",
      "\n",
      "    The join is done on columns or indexes. If joining columns on\n",
      "    columns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\n",
      "    on indexes or indexes on a column or columns, the index will be passed on.\n",
      "    When performing a cross merge, no column specifications to merge on are\n",
      "    allowed.\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        If both key columns contain rows where the key is a null value, those\n",
      "        rows will be matched against each other. This is different from usual SQL\n",
      "        join behaviour and can lead to unexpected results.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    left : DataFrame or named Series\n",
      "        First pandas object to merge.\n",
      "    right : DataFrame or named Series\n",
      "        Second pandas object to merge.\n",
      "    how : {'left', 'right', 'outer', 'inner', 'cross', 'left_anti', 'right_anti},\n",
      "        default 'inner'\n",
      "        Type of merge to be performed.\n",
      "\n",
      "        * left: use only keys from left frame, similar to a SQL left outer join;\n",
      "          preserve key order.\n",
      "        * right: use only keys from right frame, similar to a SQL right outer join;\n",
      "          preserve key order.\n",
      "        * outer: use union of keys from both frames, similar to a SQL full outer\n",
      "          join; sort keys lexicographically.\n",
      "        * inner: use intersection of keys from both frames, similar to a SQL inner\n",
      "          join; preserve the order of the left keys.\n",
      "        * cross: creates the cartesian product from both frames, preserves the order\n",
      "          of the left keys.\n",
      "        * left_anti: use only keys from left frame that are not in right frame, similar\n",
      "          to SQL left anti join; preserve key order.\n",
      "        * right_anti: use only keys from right frame that are not in left frame, similar\n",
      "          to SQL right anti join; preserve key order.\n",
      "    on : Hashable or a sequence of the previous\n",
      "        Column or index level names to join on. These must be found in both\n",
      "        DataFrames. If `on` is None and not merging on indexes then this defaults\n",
      "        to the intersection of the columns in both DataFrames.\n",
      "    left_on : Hashable or a sequence of the previous, or array-like\n",
      "        Column or index level names to join on in the left DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the left DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    right_on : Hashable or a sequence of the previous, or array-like\n",
      "        Column or index level names to join on in the right DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the right DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    left_index : bool, default False\n",
      "        Use the index from the left DataFrame as the join key(s). If it is a\n",
      "        MultiIndex, the number of keys in the other DataFrame (either the index\n",
      "        or a number of columns) must match the number of levels.\n",
      "    right_index : bool, default False\n",
      "        Use the index from the right DataFrame as the join key. Same caveats as\n",
      "        left_index.\n",
      "    sort : bool, default False\n",
      "        Sort the join keys lexicographically in the result DataFrame. If False,\n",
      "        the order of the join keys depends on the join type (how keyword).\n",
      "    suffixes : list-like, default is (\"_x\", \"_y\")\n",
      "        A length-2 sequence where each element is optionally a string\n",
      "        indicating the suffix to add to overlapping column names in\n",
      "        `left` and `right` respectively. Pass a value of `None` instead\n",
      "        of a string to indicate that the column name from `left` or\n",
      "        `right` should be left as-is, with no suffix. At least one of the\n",
      "        values must not be None.\n",
      "    copy : bool, default False\n",
      "        This keyword is now ignored; changing its value will have no\n",
      "        impact on the method.\n",
      "\n",
      "        .. deprecated:: 3.0.0\n",
      "\n",
      "            This keyword is ignored and will be removed in pandas 4.0. Since\n",
      "            pandas 3.0, this method always returns a new object using a lazy\n",
      "            copy mechanism that defers copies until necessary\n",
      "            (Copy-on-Write). See the `user guide on Copy-on-Write\n",
      "            <https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html>`__\n",
      "            for more details.\n",
      "\n",
      "    indicator : bool or str, default False\n",
      "        If True, adds a column to the output DataFrame called \"_merge\" with\n",
      "        information on the source of each row. The column can be given a different\n",
      "        name by providing a string argument. The column will have a Categorical\n",
      "        type with the value of \"left_only\" for observations whose merge key only\n",
      "        appears in the left DataFrame, \"right_only\" for observations\n",
      "        whose merge key only appears in the right DataFrame, and \"both\"\n",
      "        if the observation's merge key is found in both DataFrames.\n",
      "\n",
      "    validate : str, optional\n",
      "        If specified, checks if merge is of specified type.\n",
      "\n",
      "        * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n",
      "          left and right datasets.\n",
      "        * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n",
      "          dataset.\n",
      "        * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n",
      "          dataset.\n",
      "        * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        A DataFrame of the two merged objects.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    merge_ordered : Merge with optional filling/interpolation.\n",
      "    merge_asof : Merge on nearest keys.\n",
      "    DataFrame.join : Similar method using indices.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> df1 = pd.DataFrame(\n",
      "    ...     {\"lkey\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [1, 2, 3, 5]}\n",
      "    ... )\n",
      "    >>> df2 = pd.DataFrame(\n",
      "    ...     {\"rkey\": [\"foo\", \"bar\", \"baz\", \"foo\"], \"value\": [5, 6, 7, 8]}\n",
      "    ... )\n",
      "    >>> df1\n",
      "        lkey value\n",
      "    0   foo      1\n",
      "    1   bar      2\n",
      "    2   baz      3\n",
      "    3   foo      5\n",
      "    >>> df2\n",
      "        rkey value\n",
      "    0   foo      5\n",
      "    1   bar      6\n",
      "    2   baz      7\n",
      "    3   foo      8\n",
      "\n",
      "    Merge df1 and df2 on the lkey and rkey columns. The value columns have\n",
      "    the default suffixes, _x and _y, appended.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\")\n",
      "      lkey  value_x rkey  value_y\n",
      "    0  foo        1  foo        5\n",
      "    1  foo        1  foo        8\n",
      "    2  bar        2  bar        6\n",
      "    3  baz        3  baz        7\n",
      "    4  foo        5  foo        5\n",
      "    5  foo        5  foo        8\n",
      "\n",
      "    Merge DataFrames df1 and df2 with specified left and right suffixes\n",
      "    appended to any overlapping columns.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\", suffixes=(\"_left\", \"_right\"))\n",
      "      lkey  value_left rkey  value_right\n",
      "    0  foo           1  foo            5\n",
      "    1  foo           1  foo            8\n",
      "    2  bar           2  bar            6\n",
      "    3  baz           3  baz            7\n",
      "    4  foo           5  foo            5\n",
      "    5  foo           5  foo            8\n",
      "\n",
      "    Merge DataFrames df1 and df2, but raise an exception if the DataFrames have\n",
      "    any overlapping columns.\n",
      "\n",
      "    >>> df1.merge(df2, left_on=\"lkey\", right_on=\"rkey\", suffixes=(False, False))\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: columns overlap but no suffix specified:\n",
      "        Index(['value'], dtype='str')\n",
      "\n",
      "    >>> df1 = pd.DataFrame({\"a\": [\"foo\", \"bar\"], \"b\": [1, 2]})\n",
      "    >>> df2 = pd.DataFrame({\"a\": [\"foo\", \"baz\"], \"c\": [3, 4]})\n",
      "    >>> df1\n",
      "          a  b\n",
      "    0   foo  1\n",
      "    1   bar  2\n",
      "    >>> df2\n",
      "          a  c\n",
      "    0   foo  3\n",
      "    1   baz  4\n",
      "\n",
      "    >>> df1.merge(df2, how=\"inner\", on=\"a\")\n",
      "          a  b  c\n",
      "    0   foo  1  3\n",
      "\n",
      "    >>> df1.merge(df2, how=\"left\", on=\"a\")\n",
      "          a  b  c\n",
      "    0   foo  1  3.0\n",
      "    1   bar  2  NaN\n",
      "\n",
      "    >>> df1 = pd.DataFrame({\"left\": [\"foo\", \"bar\"]})\n",
      "    >>> df2 = pd.DataFrame({\"right\": [7, 8]})\n",
      "    >>> df1\n",
      "        left\n",
      "    0   foo\n",
      "    1   bar\n",
      "    >>> df2\n",
      "        right\n",
      "    0   7\n",
      "    1   8\n",
      "\n",
      "    >>> df1.merge(df2, how=\"cross\")\n",
      "       left  right\n",
      "    0   foo      7\n",
      "    1   foo      8\n",
      "    2   bar      7\n",
      "    3   bar      8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6bce9-70d4-488b-a4f7-1eff5d0150f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Tm</th>\n",
       "      <th>2025-26</th>\n",
       "      <th>Rk</th>\n",
       "      <th>Age</th>\n",
       "      <th>Team</th>\n",
       "      <th>Pos</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>...</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "      <th>Trp-Dbl</th>\n",
       "      <th>Awards</th>\n",
       "      <th>Player-additional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Garrison Mathews</td>\n",
       "      <td>IND</td>\n",
       "      <td>$131,970</td>\n",
       "      <td>398.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>SG</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mathega01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Garrison Mathews</td>\n",
       "      <td>IND</td>\n",
       "      <td>$131,970</td>\n",
       "      <td>398.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>SG</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mathega01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac McClung</td>\n",
       "      <td>IND</td>\n",
       "      <td>$164,060</td>\n",
       "      <td>459.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2TM</td>\n",
       "      <td>SG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mccluma01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mac McClung</td>\n",
       "      <td>IND</td>\n",
       "      <td>$164,060</td>\n",
       "      <td>459.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>SG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mccluma01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mac McClung</td>\n",
       "      <td>IND</td>\n",
       "      <td>$164,060</td>\n",
       "      <td>459.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>SG</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mccluma01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player   Tm    2025-26     Rk   Age Team Pos     G   GS     MP  \\\n",
       "0  Garrison Mathews  IND  $131,970   398.0  29.0  IND  SG  15.0  1.0  196.0   \n",
       "1  Garrison Mathews  IND  $131,970   398.0  29.0  IND  SG  15.0  1.0  196.0   \n",
       "2       Mac McClung  IND  $164,060   459.0  27.0  2TM  SG   4.0  0.0   47.0   \n",
       "3       Mac McClung  IND  $164,060   459.0  27.0  IND  SG   3.0  0.0   34.0   \n",
       "4       Mac McClung  IND  $164,060   459.0  27.0  CHI  SG   1.0  0.0   13.0   \n",
       "\n",
       "   ...   TRB   AST  STL  BLK  TOV    PF   PTS  Trp-Dbl  Awards  \\\n",
       "0  ...  17.0  10.0  6.0  3.0  3.0  19.0  78.0      0.0     NaN   \n",
       "1  ...  17.0  10.0  6.0  3.0  3.0  19.0  78.0      0.0     NaN   \n",
       "2  ...   5.0   2.0  5.0  2.0  3.0   8.0  23.0      0.0     NaN   \n",
       "3  ...   4.0   1.0  5.0  1.0  2.0   6.0  19.0      0.0     NaN   \n",
       "4  ...   1.0   1.0  0.0  1.0  1.0   2.0   4.0      0.0     NaN   \n",
       "\n",
       "   Player-additional  \n",
       "0          mathega01  \n",
       "1          mathega01  \n",
       "2          mccluma01  \n",
       "3          mccluma01  \n",
       "4          mccluma01  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.merge(salary_data, stats, on='Player')\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355a8ca-5066-4be9-a573-ed8a3d56ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Player   Tm    2025-26     Rk   Age Team Pos     G   GS  \\\n",
      "0          Garrison Mathews  IND  $131,970   398.0  29.0  IND  SG  15.0  1.0   \n",
      "1          Garrison Mathews  IND  $131,970   398.0  29.0  IND  SG  15.0  1.0   \n",
      "2               Mac McClung  IND  $164,060   459.0  27.0  2TM  SG   4.0  0.0   \n",
      "3               Mac McClung  IND  $164,060   459.0  27.0  IND  SG   3.0  0.0   \n",
      "4               Mac McClung  IND  $164,060   459.0  27.0  CHI  SG   1.0  0.0   \n",
      "..                      ...  ...        ...    ...   ...  ...  ..   ...  ...   \n",
      "518  Jeremiah Robinson-Earl  IND        NaN  383.0  25.0  IND  PF  17.0  3.0   \n",
      "519  Jeremiah Robinson-Earl  IND        NaN  383.0  25.0  DAL  PF   5.0  0.0   \n",
      "520  Jeremiah Robinson-Earl  IND        NaN  383.0  25.0  2TM  PF  22.0  3.0   \n",
      "521  Jeremiah Robinson-Earl  IND        NaN  383.0  25.0  IND  PF  17.0  3.0   \n",
      "522  Jeremiah Robinson-Earl  IND        NaN  383.0  25.0  DAL  PF   5.0  0.0   \n",
      "\n",
      "        MP  ...    TRB   AST  STL  BLK   TOV    PF    PTS  Trp-Dbl  Awards  \\\n",
      "0    196.0  ...   17.0  10.0  6.0  3.0   3.0  19.0   78.0      0.0     NaN   \n",
      "1    196.0  ...   17.0  10.0  6.0  3.0   3.0  19.0   78.0      0.0     NaN   \n",
      "2     47.0  ...    5.0   2.0  5.0  2.0   3.0   8.0   23.0      0.0     NaN   \n",
      "3     34.0  ...    4.0   1.0  5.0  1.0   2.0   6.0   19.0      0.0     NaN   \n",
      "4     13.0  ...    1.0   1.0  0.0  1.0   1.0   2.0    4.0      0.0     NaN   \n",
      "..     ...  ...    ...   ...  ...  ...   ...   ...    ...      ...     ...   \n",
      "518  300.0  ...   88.0  12.0  7.0  1.0  10.0  23.0   79.0      0.0     NaN   \n",
      "519   61.0  ...   15.0   3.0  2.0  0.0   2.0   3.0   22.0      0.0     NaN   \n",
      "520  361.0  ...  103.0  15.0  9.0  1.0  12.0  26.0  101.0      0.0     NaN   \n",
      "521  300.0  ...   88.0  12.0  7.0  1.0  10.0  23.0   79.0      0.0     NaN   \n",
      "522   61.0  ...   15.0   3.0  2.0  0.0   2.0   3.0   22.0      0.0     NaN   \n",
      "\n",
      "     Player-additional  \n",
      "0            mathega01  \n",
      "1            mathega01  \n",
      "2            mccluma01  \n",
      "3            mccluma01  \n",
      "4            mccluma01  \n",
      "..                 ...  \n",
      "518          robinje02  \n",
      "519          robinje02  \n",
      "520          robinje02  \n",
      "521          robinje02  \n",
      "522          robinje02  \n",
      "\n",
      "[166 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicates = merged_data[merged_data.duplicated(subset='Player', keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No kernel connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No kernel connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No kernel connected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
